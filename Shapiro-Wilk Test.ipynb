{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Shapiro-Wilk Normacy Test\n",
    "\n",
    "\n",
    "In these notes, I want to walk through the reasoning behind the original [*An analysis of variance test for normality* 1965 paper by Shapiro and Wilk](https://sci2s.ugr.es/keel/pdf/algorithm/articulo/shapiro1965.pdf). This paper is the first to propose what is now known as the Shapiro-Wilk test of normality. I came accross this test while trying to learn more about time series statistics, but I had a hard time finding any information about the test. It turns out the best documentation on the test was the original paper. I am posting this because it was difficult for me to track down information about the Shapiro-Wilk test online, and maybe it will save some time for some readers.\n",
    "\n",
    "The best information I *was* able to find came from [this blog post](https://www.real-statistics.com/tests-normality-and-symmetry/statistical-tests-normality-symmetry/shapiro-wilk-test/) listing the steps of the computation and this [Fortran implementation](http://lib.stat.cmu.edu/apstat/R94). In fact that later implementation is the one that [SciPy calls](https://github.com/scipy/scipy/blob/master/scipy/stats/morestats.py#L1609) to implement the test.\n",
    "\n",
    "The [Wikipedia article](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test) on the test lists a simple formula:\n",
    "\n",
    "$$ W = \\frac{\\big(\\sum_{n=1}^N a_i x_i\\big)^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\vec{a} = \\frac{m^T V^{-1}}{C} $$\n",
    "\n",
    "where \n",
    "\n",
    "$$ C = \\lVert V^{-1}m \\rVert $$\n",
    "\n",
    "where\n",
    "\n",
    "$m$ \"is made of the expected values of the order statistics of independent and identically distributed random variables sampled from the standard normal distribution; finally, $V$ is the covariance matrix of those normal order statistics\".\n",
    "\n",
    "None of that looks too hard to calculate. This could be done in a few lines with [NumPy](https://numpy.org/), but we need to decode the meaning of $m$ and $V$. It is not at all clear how the magic numbers used in the Fortran implementation correspond to this description. (In fact, even at the time of posting, I still do not understand the source for those numbers.)\n",
    "\n",
    "TODO(gs): Figure out and document how the R94 Shapiro-Wilk implementation works.\n",
    "\n",
    "It looks like we need to go back to the original paper.\n",
    "\n",
    " I will not use any libraries other than SciPy and [Matplotlib](https://matplotlib.org/).\n",
    "However, SciPy use will be restricted to the `array` and `linalg` modules. \n",
    "\n",
    "Now to calculate the W statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order Statistics\n",
    "\n",
    "*(Some of this section is pulled from [Kim Border's lecture notes](http://www.math.caltech.edu/~2016-17/2term/ma003/Notes/Lecture14.pdf).)*\n",
    "\n",
    "We follow the definition in the paper exactly. The first step is to calculate the order statistics.\n",
    " \n",
    "\"Let $m^\\prime = (m_1,m_2,..., m_n)$ denote the vector of expected values of the standard normal order statistics.\"\n",
    "\n",
    "The $k$-th order statistic for a sample is given by the $k$-th least element of the sample. We wish to calculate an expected value for an order statistic. In particular, we are looking for an expression to find the expected value for the $k$-th order statistic from a sample of $n$ elements from a probability distribution given by PDF $\\rho$. Call the random variable for this order statistic $X_{k,n}$, and the underlying random variable $X$. \n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$ (X_{k,n} \\leq x) = (X_{n,n} \\leq x) \\cup (X_{n,n} > x, X_{n-1,n} \\leq x) \\cup ... \\cup (X_{k+1,n} > x, X_{k,n} \\leq x)$$\n",
    "\n",
    "Each of these conditions is disjoint and therefore we can add the probabilities to find the PDF for $X_{k,n}$.\n",
    "\n",
    "Before moving on, we should note that\n",
    "\n",
    "$$ P(X \\leq x) = \\int_{-\\infty}^x d\\tau \\rho(x) $$\n",
    "\n",
    "We can simplify by defining $F(x) \\equiv P(X \\leq x)$ so that $F^\\prime(x) = \\rho(x)$ and $F$ is the CDF of the distribution.\n",
    "\n",
    "$$ P(X_{n,n} \\leq x) = P(X \\leq x)^n = F(x)^n $$\n",
    "\n",
    "$$ P(X_{n,n} > x, X_{n-1,n} \\leq x) = {n \\choose 1} \\big(1 - F(x)\\big)F(x)^{n-1} $$\n",
    "\n",
    "$$ P(X_{n-1,n} > x, X_{n-2,n} \\leq x) = {n \\choose 2} \\big(1 - F(x)\\big)^2F(x)^{n-2} $$\n",
    "\n",
    "$$...$$\n",
    "\n",
    "$$ P(X_{k+1,n} > x, X{k, n} \\leq x) = {n \\choose {n-k}} \\big(1-F(x)\\big)^{n-k}F(x)^k $$\n",
    "\n",
    "Therefore, if $F_{k,n}$ is the CDF of the $k$-th order statistic, then\n",
    "\n",
    "$$ F_{k, n}(x) = \\sum_{i=k}^n {n \\choose i} \\big(1-F(x)\\big)^{n-i}F(x)^i $$\n",
    "\n",
    "And if $f_{k,n}$ is the PDF for the same distribution, we can derive the expression for $F_{k,n}$ to get\n",
    "\n",
    "$$ f_{k,n}(x) = \\frac{dF_{k, n}}{dx} = n {{n-1} \\choose {k-1}} \\big(1-F(x)\\big)^{n-k}F(x)^{k-1}f(x) $$\n",
    "\n",
    "In theory we would integrate over $f_{k,n}$ to find the expected value, but instead we can use a numeric method to find $x$ such that $F_{k,n} = \\frac{1}{2}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple test passed!\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "def factorial(n: int) -> int:\n",
    "    \"\"\"Compute the factorial for n i.e. n!\n",
    "    \n",
    "    Args:\n",
    "        n: the factorial to compute.\n",
    "        \n",
    "    Returns:\n",
    "        The factorial of n.\n",
    "    \"\"\"\n",
    "    if n < 0:\n",
    "        raise ValueError()\n",
    "    ret = 1\n",
    "    for k in range(1, n+1):\n",
    "        ret *= k\n",
    "    return ret\n",
    "\n",
    "def choose(k: int, n: int) -> float:\n",
    "    \"\"\"Calculate \"n choose k\".\n",
    "    \n",
    "    Args:\n",
    "        k: the number of elements to choose\n",
    "        n: the number of elements from which to choose\n",
    "    \n",
    "    Returns:\n",
    "        The number of possible combinations.\n",
    "    \"\"\"\n",
    "    return float(factorial(n) / factorial(k) / factorial(n-k))\n",
    "\n",
    "def order_statistic(k: int, n: int, cdf: Callable[[float], float], err: float = 0.00001) -> float:\n",
    "    \"\"\"Calcualte the expected value of the the order statistics for a sample\n",
    "    of n members drawn from a probability distribution defined by CDF cdf.\n",
    "    \n",
    "    This is only guaranteed if the CDF is almost always increasing.\n",
    "    \n",
    "    Args:\n",
    "        k: the order statistic\n",
    "        n: the size of the sample\n",
    "        cdf: the CDF of the function\n",
    "    \n",
    "    Returns:\n",
    "        The expected value of the k-th order statistic of a sample of n\n",
    "        members drawn from a distributions defined by the CDF. \n",
    "    \"\"\"\n",
    "    check = lambda x: sum([choose(i, n) * (1 - cdf(x))**(n-i)*cdf(x)**i for i in range(k, n+1)])\n",
    "    hi = 0.\n",
    "    while check(hi) < 0.5:\n",
    "        hi += 1\n",
    "    lo = 0.\n",
    "    while check(lo) > 0.5:\n",
    "        lo -= 1\n",
    "    while hi - lo > err:\n",
    "        mid = (hi + lo) / 2.\n",
    "        found = check(mid)\n",
    "        if found > 0.5:\n",
    "            hi = mid\n",
    "            continue\n",
    "        if found < 0.5:\n",
    "            lo = mid\n",
    "            continue\n",
    "        return mid\n",
    "    return (hi + lo) / 2.\n",
    "\n",
    "from math import erf, fabs\n",
    "from random import gauss\n",
    "\n",
    "def test_order_statistic():\n",
    "    \"\"\"Test that the order statistic code works as expected.\"\"\"\n",
    "    okay = True\n",
    "    err = 0.001\n",
    "    cdf = lambda x: 0.5 + 0.5*erf(x / (2**0.5))\n",
    "    max_iter = 100\n",
    "    for n in range(7, 10): \n",
    "        for k in range(2, n+1):\n",
    "            stat = 0.\n",
    "            for itr in range(max_iter):\n",
    "                sample = [gauss(0, 1) for _i in range(n)]\n",
    "                sample = sorted(sample)\n",
    "                stat += sample[k-1]\n",
    "            stat /= float(max_iter)\n",
    "            if fabs(order_statistic(k, n, cdf) - stat) < err:\n",
    "                okay = False\n",
    "                print(\"Error for n={}, k={}\".format(n, k))\n",
    "        \n",
    "    if okay:\n",
    "        print(\"Simple test passed!\")\n",
    "    else:\n",
    "        print(\"Could not calculate order statistic correctly.\")\n",
    "\n",
    "test_order_statistic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to calculate the expected covariance of the order statistics.\n",
    "\n",
    "\"Let $V= (v_{ij})$ be the corresponding $n\\times n$ covariance matrix.\"\n",
    "\n",
    "Unfortunately, it is not clear that there is a closed form expression for the covariance. I am not sure if there are any methods that would allow for exact computation other than numeric integrals. However, we can run a Monte-Carlo simulation for the Gaussian case, the one we are interested in for the Shapiro-Wilk test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from math import floor\n",
    "\n",
    "def covariance(pairs: List[Tuple[float, float]]) -> float:\n",
    "    \"\"\"Find the sample covariance of a sample.\n",
    "    \n",
    "    Args:\n",
    "        pairs: the data for which to calculate the covariace\n",
    "        \n",
    "    Returns:\n",
    "        The sample covariance.\n",
    "    \"\"\"\n",
    "    N = float(len(pairs))\n",
    "    x_bar = sum([pair[0] for pair in pairs]) / N\n",
    "    y_bar = sum([pair[1] for pair in pairs]) / N\n",
    "    return sum([(pair[0] - x_bar)*(pair[1] - y_bar) for pair in pairs]) / N\n",
    "\n",
    "def order_statistic_gauss_covariance(i: int, j: int, n: int, batch_size: int = 10000) -> float:\n",
    "    \"\"\"Calculate the covariance for two order statistics drawn from a Gaussian distribution.\n",
    "    \n",
    "    Args:\n",
    "        i: the first order statistic index, must be less than or equal to j, n\n",
    "        j: the second order statistic index, must be greater than or equal to i and less than or equal to n\n",
    "        n: the number of elements in the sample, must be greater than or equal to i, j\n",
    "        batch_size: the number of iterations used to calculate the covarience\n",
    "    \n",
    "    Returns:\n",
    "        An estimate for the covarience\n",
    "    \"\"\"\n",
    "    samples: List[Tuple[float, float]] = []\n",
    "    for _k in range(batch_size):\n",
    "        one_sample = [gauss(0, 1) for _kk in range(n)]\n",
    "        one_sample = sorted(one_sample)\n",
    "        samples.append((one_sample[i-1], one_sample[j-1]))\n",
    "    return covariance(samples)\n",
    "        \n",
    "def print_covariance_table() -> None:\n",
    "    n = 10\n",
    "    for i in range(1, n+1):\n",
    "        print(\"i={},j=1..{}: \".format(i, n), [(order_statistic_gauss_covariance(i, j, 10) \\\n",
    "                                    if i < j else order_statistic_gauss_covariance(j, i, 10)) for j in range(1, n+1)])\n",
    "    \n",
    "\n",
    "# print_covariance_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you print the covariance table a few times, you will see that even with 10,000 samples, the covarience estimations can change by several percentage points. We can plot a histogram of estimates for i=1, j=1, n=10, e.g. the varience of $X_{1,10}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_covariance_estimate(i: int, j: int, n: int, nsamples: int = 1000) -> None:\n",
    "    \"\"\"Plot a histogram of covariance estimates.\n",
    "    \n",
    "    Args:\n",
    "        i: the first order statistic index\n",
    "        j: the second order statistic index\n",
    "        nsamples: the number of samples to take to calculate the statistic\n",
    "    \"\"\"\n",
    "    estimates = []\n",
    "    for _k in range(nsamples):\n",
    "        estimates.append(order_statistic_gauss_covariance(i, j, n, batch_size=1000))\n",
    "    plt.hist(estimates, bins=50)\n",
    "    plt.title(r\"Estimated values of $cov(X_{1,10}, X_{1,10})$, e.g. $var(X_{1,10})$\")\n",
    "    plt.xlabel(\"Estimated covarience\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_covariance_estimate(1, 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the W statistic\n",
    "\n",
    "The estimated population mean is given as usual by $\\hat{\\mu} = N^{-1} \\sum_{i=1}^N x_i$. However, the standard deviation is given by a more complex formula. I am sure some professionals are familiar with this result, but it was my first time encountering it, so I will include a more detailed discussion.\n",
    "\n",
    "The authors cite two previous papers [*On least squares and linear combination of observations*, a 1935 paper by Alexander Aitken](https://www.cambridge.org/core/journals/proceedings-of-the-royal-society-of-edinburgh/article/abs/ivon-least-squares-and-linear-combination-of-observations/7106C26F19F2EBF75BCEE7FA285780B9), and [*Least squares estimation of location and scale parameters using order statistics*, a 1952 paper by E.H. Lloyd](https://www.jstor.org/stable/pdf/2332466.pdf).\n",
    "\n",
    "\n",
    "I was not able to find a copy of the Aitken paper other than Cambridge's pay-walled page. As far as I can tell, it introduces the idea of a generalized least squares regression for fitting models. [Aitken himself](https://en.wikipedia.org/wiki/Alexander_Aitken) is also interesting, being New Zeland's most famous mathematician. I don't believe any one still reading is unfamiliar with the idea of least squares regression, so I will move on to the Lloyd paper.\n",
    "\n",
    "On the other hand, I could find a copy of the Lloyd paper on Jstor, but I could not find any biographic information about Lloyd. I am not even sure what \"E.H.\" stands for. \n",
    "\n",
    "## Digression: The Lloyd paper\n",
    "\n",
    "The Lloyd paper introduces a method for calculating the mean and standard deviation of symmetric distributions. Say we have a stochastic variable $X$ with a symmetric probability distribution about some mean $\\mu$. We can normalize the width of this distribution with a parameter $\\sigma$. Therefore, we can rewrite the value $X = \\mu + \\sigma U$ where $U$ is a stochastic variable with a symmetric distribution about the origin. If we take a sample of $N$ elements from $X$ and order them into $Y_1, Y_2, ..., Y_N$ so that $Y_i \\leq Y_{i+1}$, we can construct the series $Z_1, Z_2, ..., Z_N$ where $Z_i = (Y_i - \\mu) / \\sigma$.\n",
    "\n",
    "Define $\\alpha_i \\equiv E[Z_i]$ and $\\Omega_{i,j} \\equiv cov(Z_i, Z_j)$. Even if the normalized distribution for $U$ is known, but not the parameters $\\mu$ and $\\sigma$, one can still calculate the values for $\\alpha_i$ and $\\Omega_{i,j}$.\n",
    "\n",
    "Here is the clever bit: We then know that $E[Y_i] = \\mu + \\sigma \\alpha_i$, $var(Y_i) = \\sigma^2\\Omega_{i, i}$ and $cov(T_i, Y_j) = \\sigma^2\\Omega_{i,j}$. By taking the square root of the later two expressions, we how have a three linear equations linear in $\\mu$ and $\\sigma$ and can use linear least squares regression to estimate their value. Note: this does not depend on the distribution $U$ being normal, only that it is symmetric.\n",
    "\n",
    "We can rewrite the above as a vector sum,\n",
    "\n",
    "$$ E[\\overrightarrow{Y}] = \\mu \\overrightarrow{1} + \\sigma \\overrightarrow{\\alpha} $$\n",
    "\n",
    "where $\\overrightarrow{Y}$ and $\\overrightarrow{\\alpha}$ are vectors of the $Y_i$ and $\\alpha_i$ values and $\\overrightarrow{1}$ is a vector of 1's.\n",
    "\n",
    "Define the matrix $p = [\\overrightarrow{1} \\overrightarrow{\\alpha}]$ as the $(n\\times 2)$ matrix and the vector $\\overrightarrow{\\theta} = [\\mu, \\sigma]^T$. Therefore $E[\\overrightarrow{Y}] = p\\overrightarrow{\\theta}$\n",
    "\n",
    "We can also express the covariance matrix for the $Y_i$ as $V[\\overrightarrow{Y}] = \\sigma^2 \\Omega$ where $\\Omega$ is the matrix of the $\\Omega_{i,j}$ values.\n",
    "\n",
    "If we estimate $\\theta$ as $\\overrightarrow{Y} = p\\hat{\\theta}$ and multiply both sides by $p^T\\Omega^{-1}$ and rearrange we get\n",
    "\n",
    "$$ \\hat{\\theta} = \\Big(p^T \\Omega^{-1} p\\Big)^{-1} p^T\\Omega^{-1} \\overrightarrow{Y} $$\n",
    "\n",
    "We can calculate \n",
    "\n",
    "$$ p^T\\Omega^{-1}p = \\begin{bmatrix} \\overrightarrow{1}^T\\Omega^{-1}\\overrightarrow{1} & \\overrightarrow{1}^T\\Omega^{-1}\\overrightarrow{\\alpha} \\\\ \n",
    "\\overrightarrow{1}^T\\Omega^{-1}\\overrightarrow{\\alpha} & \\overrightarrow{\\alpha}^T\\Omega^{-1}\\overrightarrow{\\alpha}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "And therefore\n",
    "\n",
    "$$ \\Big(p^T\\Omega^{-1}p\\Big)^{-1} = \\frac{1}{\\Delta}\\begin{bmatrix} \\overrightarrow{\\alpha}^T\\Omega^{-1}\\overrightarrow{\\alpha} & \n",
    "-\\overrightarrow{1}^T\\Omega^{-1}\\overrightarrow{\\alpha} \\\\ \n",
    "-\\overrightarrow{1}^T\\Omega^{-1}\\overrightarrow{\\alpha} &\n",
    "\\overrightarrow{1}^T\\Omega^{-1}\\overrightarrow{1}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\Delta = det\\Big(p^T\\Omega^{-1}p\\Big) = \\overrightarrow{1}^T\\Omega^{-1}\\overrightarrow{1} \\overrightarrow{\\alpha}^T\\Omega^{-1}\\overrightarrow{\\alpha} - (\\overrightarrow{1}^T\\Omega^{-1}\\overrightarrow{\\alpha})^2 $$\n",
    "\n",
    "We can already substitute into the expression for $\\hat{\\theta}$ to get a closed form expression, but let's keep simplifying.\n",
    "\n",
    "Let $J$ be a sparse skew-symmetric matrix with 1's along the reverse diagonal. Thus $J = J^T = J^{-1}$. Further $J \\overrightarrow{1} = \\overrightarrow{1}$\n",
    "\n",
    "Multiplying a vector by $J$ reverses the order of its elements. For symmetric distributions, $J \\overrightarrow{\\alpha} = -\\overrightarrow{\\alpha}$ and $V[\\overrightarrow{Y}] = V[-J\\overrightarrow{Y}] = \\Omega$. Finally note that, $J\\Omega J = \\Omega$ because $\\Omega$ is skew-symmetric, and similarly $J\\Omega^{-1}J = \\Omega^{-1}$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$ \\overrightarrow{1}^T\\Omega^{-1}\\overrightarrow{\\alpha} = \n",
    "\\overrightarrow{1}^T(J\\Omega^{-1}J)(-J\\overrightarrow{\\alpha}) =\n",
    "-(\\overrightarrow{1}^TJ)\\Omega(J^2)\\overrightarrow{\\alpha} =\n",
    "-\\overrightarrow{1}^T\\Omega\\overrightarrow{\\alpha}\n",
    "$$\n",
    "\n",
    "So that $\\overrightarrow{1}^T\\Omega^{-1}\\overrightarrow{\\alpha} = 0$.\n",
    "\n",
    "For symmetric distributions, $\\overrightarrow{1}^T\\Omega^{-1}\\overrightarrow{\\alpha} = 0$, so the second term in $\\Delta$ is zero and \n",
    "\n",
    "$$ \\Delta = \\overrightarrow{1}^T\\Omega^{-1}\\overrightarrow{1} \\overrightarrow{\\alpha}^T\\Omega^{-1}\\overrightarrow{\\alpha}$$\n",
    "\n",
    "This simplifies the expressions for $\\hat{\\mu}$ and $\\hat{\\sigma}$ to \n",
    "\n",
    "$$ \\hat{\\mu} = \\frac{\\overrightarrow{1}^T\\Omega^{-1}\\overrightarrow{Y}}{\\overrightarrow{1}^T\\Omega^{-1}\\overrightarrow{1}} $$\n",
    "\n",
    "$$ \\hat{\\sigma} = \\frac{\\overrightarrow{\\alpha}^T\\Omega^{-1}\\overrightarrow{Y}}{\\overrightarrow{\\alpha}^T\\Omega^{-1}\\overrightarrow{\\alpha}} $$\n",
    "\n",
    "I should note here that in the Shapiro-Wilk paper, the authors write that \n",
    "\n",
    "$$ \\hat{\\mu} = \\frac{\\sum_{i=1}^N Y_i}{N} $$\n",
    "\n",
    "instead of \n",
    "\n",
    "$$ \\hat{\\mu} = \\frac{\\sum_{i,j=1}^N \\omega_{i,j}^{(-1)} Y_j} {\\sum_{i,j=1}^N \\omega_{i,j}^{(-1)}} $$\n",
    "\n",
    "This may be an error. Intuitively is seems the authors assertion should be correct for normal distributions, but I could not replicate the result. For it to be true, it must be the case that \n",
    "\n",
    "$$ N^{-1} = \\frac{\\sum_{i=1}^N \\omega_{i,j^\\prime}^{(-1)}}{\\sum_{i,j=1}^N \\omega_{i,j}^{(-1)}}  $$\n",
    "\n",
    "for all $j^\\prime \\in \\{1, 2, ..., N\\}$. If there exists an $A$ such that for all $j^\\prime \\in \\{1, 2, ..., N\\}$ $\\sum_{i=1}^N \\omega_{i,j^\\prime}^{(-1)} = A$, then the result would hold. \n",
    "\n",
    "<!-- see this: https://math.stackexchange.com/questions/2936028/inverse-matrix-sum-of-the-elements-in-each-row -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to the W statistic\n",
    "\n",
    "We can then define the W statistic as\n",
    "\n",
    "$$ W = \\frac{\\Big(\\overrightarrow{\\alpha}^T\\Omega^{-1}\\overrightarrow{Y}\\Big)^2}{\\lVert\\overrightarrow{\\alpha}^T \\Omega^{-1}\\rVert^2\\sum_{i=1}^N (Y_i - \\bar{Y})^2} $$\n",
    "\n",
    "We can define\n",
    "\n",
    "$$ S^2 = \\sum_{i=1}^N \\Big(Y_i - \\bar{Y}\\Big)^2 $$\n",
    "\n",
    "$$ \\hat{a}^T = \\frac{\\overrightarrow{\\alpha}^T\\Omega^{-1}}{\\lVert\\overrightarrow{\\alpha}^T\\Omega^{-1}\\rVert} $$\n",
    "\n",
    "so that we can write $ W = \\Big(\\hat{a}^T \\overrightarrow{Y}\\Big)^2 / S^2 $. First, note that the value of $\\hat{a}$ only depends on the value of $N$, the number of elements in the sample. As far as I can tell, there is no English explanation of how the statistic works that would lead to any intuition other than the following analysis. I believe this is part of the reason it is still called the \"Shapiro-Wilk W statistic\".\n",
    "\n",
    "The statistic for a given sample is invariant to translations and scaling. Intuitively this makes sense because it should be invariant to changes in mean and standard deviation. \n",
    "\n",
    "However, we can explicitly show this for the mean by noting that $J\\hat{a} = - \\hat{a}$ so without loss of generality, we can define $\\overrightarrow{Y}^\\prime = \\overrightarrow{Y} + \\lambda\\overrightarrow{1}$ so $\\bar{Y}^\\prime = \\bar{Y} + \\lambda$. Therefore\n",
    "\n",
    "$$ W^\\prime = \\frac{\\Big(\\hat{a}^T \\overrightarrow{Y}^\\prime\\Big)^2}{S^{\\prime2}} $$\n",
    "\n",
    "But\n",
    "\n",
    "$$ S^{\\prime2} = \\sum_{i=1}^N \\Big( Y_i^\\prime - \\bar{Y}^\\prime \\Big)^2 = \\\n",
    "\\sum_{i=1}^N \\Big( (Y_i + \\lambda) - (\\bar{Y} + \\lambda) \\Big)^2 = \\\n",
    "\\sum_{i=1}^N \\Big( Y_i - \\bar{Y} \\Big)^2 = S^2\n",
    "$$\n",
    "\n",
    "And\n",
    "\n",
    "$$ \\hat{a}^T\\overrightarrow{Y}^\\prime = \\hat{a}^T(\\overrightarrow{Y} + \\lambda \\overrightarrow{1}) = \\hat{a}^T\\overrightarrow{Y} + \\lambda \\hat{a}^T\\overrightarrow{1} = \\hat{a}^T\\overrightarrow{Y}\n",
    "$$\n",
    "\n",
    "The last step holds because \n",
    "\n",
    "$$ \\hat{a}^T\\overrightarrow{1} = -(\\hat{a}^TJ)\\overrightarrow{1} = \n",
    "-\\hat{a}^T(J\\overrightarrow{1}) = -\\hat{a}^T\\overrightarrow{1} $$\n",
    "\n",
    "and thus $\\hat{a}^T\\overrightarrow{1} = 0$.\n",
    "\n",
    "Substituting the above:\n",
    "\n",
    "$$ W^\\prime = \\frac{\\Big(\\hat{a}^T \\overrightarrow{Y}^\\prime\\Big)^2}{S^{\\prime2}} =\n",
    "\\frac{\\Big(\\hat{a}^T \\overrightarrow{Y}\\Big)^2}{S^2} = W\n",
    "$$\n",
    "\n",
    "Now we show that the statistic is scale invariant. Suppose without loss of generality that $\\overrightarrow{Y}^\\prime = \\lambda\\overrightarrow{Y}$. Then \n",
    "\n",
    "$$ W^\\prime = \\frac{\\Big(\\hat{a}^T \\overrightarrow{Y}^\\prime\\Big)^2}{S^{\\prime2}} = \\frac{\\Big(\\hat{a}^T (\\lambda \\overrightarrow{Y})\\Big)^2}{\\lambda^2S^2} = \\frac{\\lambda^2\\Big(\\hat{a}^T \\overrightarrow{Y}\\Big)^2}{\\lambda^2S^2} = \\frac{\\Big(\\hat{a}^T \\overrightarrow{Y}\\Big)^2}{S^2} = W $$\n",
    "\n",
    "Finally we can show the maximum value of $W$ is 1.\n",
    "\n",
    "Suppose without loss of generality due to translation invariance, that $\\bar{Y} = 0$. Then $W = (\\hat{a}^T\\overrightarrow{Y})^2 / \\sum_{i=1}^N Y_i^2$. However, \n",
    "\n",
    "$$ \\Big(\\hat{a}^T \\overrightarrow{Y}\\Big)^2 = \n",
    "\\Big(\\sum_{i=1}^N a_i Y_i \\Big)^2 \\leq \n",
    "\\sum_{i=1}^N a_i^2 \\sum_{j=1}^N Y_j^2 =\n",
    "\\sum_{i=1}^N Y_i^2\n",
    "$$\n",
    "\n",
    "where $a_i$ is the $i$th element of $\\hat{a}$. The last step is true because \n",
    "$\\lVert \\hat{a} \\rVert = 1$. The maximum value is clearly when $\\overrightarrow{Y} = \\lambda \\hat{a}$ for some scalar $\\lambda$.\n",
    "\n",
    "This is important because we can write the above Lloyd estimate for population standard deviation as \n",
    "\n",
    "$$ \\hat{\\sigma} = C\\hat{a}^T\\overrightarrow{Y} $$\n",
    "\n",
    "where $C$ is a normalization constant. To maximize $\\hat{\\sigma}$ for a given $S^2$ requires that $\\overrightarrow{Y} = \\lambda \\hat{a}$ for some $\\lambda$. Because $\\Omega^{-1}$ a concentration matrix, we are finding how much variation in the expected value for the order statistics can be explained by natural variation. But because the distribution of $W$ cannot analytically estimated, we must defer to the Monte-Carlo simulations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "\n",
    "def order_statistic_gauss_covariance_matrix(N: int, batch_size: int = 10000):\n",
    "    \"\"\"Calculate the order statistic covariance matrix for a normal distribution.\n",
    "    \n",
    "    Args:\n",
    "        N: the sample size\n",
    "        batch_size: the size of the batches used in the Monte-Carlo simulation\n",
    "    \n",
    "    Returns:\n",
    "        An np.array containing the covariance matrix.\n",
    "    \"\"\"\n",
    "    return np.array([[order_statistic_gauss_covariance(row, col, N, batch_size=batch_size) for col in range(N)]\n",
    "                     for row in range(N)])\n",
    "\n",
    "def order_statistic_gauss_precision_matrix(N: int, batch_size: int = 10000):\n",
    "    \"\"\"Calculate  the order statistic precission matrix for a normal distribution.\n",
    "    \n",
    "    Args:\n",
    "        N: the sample size\n",
    "        batch_size: the size of the batches used in the Monte-Carlo simulation\n",
    "    \n",
    "    Returns:\n",
    "        An np.array containing the precission matrix.\n",
    "    \"\"\"\n",
    "    return linalg.inv(order_statistic_gauss_covariance_matrix(N, batch_size=batch_size))\n",
    "\n",
    "\n",
    "def expected_order_statistic_gauss(i: int, N: int) -> float:\n",
    "    \"\"\"Get the expected value for the i-th order statistic.\n",
    "    \n",
    "    Args:\n",
    "        i: the index of the order statistic\n",
    "        N: the number of elements in the sample\n",
    "    \"\"\"\n",
    "    return order_statistic(i, N, lambda x: 0.5 + 0.5*erf(x / (2**0.5)))\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "def shapiro_wilk_coefficients(N: int, batch_size: int = 10000):\n",
    "    \"\"\"Calculate the Shapiro-Wilk coefficients.\n",
    "    \n",
    "    Args:\n",
    "        N: the number of samples\n",
    "        batch_size: the size of the batched used int he Monte-Carlo simulation\n",
    "        \n",
    "    Returns:\n",
    "        The coefficients as an np.array.\n",
    "    \"\"\"\n",
    "    mat = order_statistic_gauss_precision_matrix(N, batch_size=batch_size)\n",
    "    order_stats = np.array([expected_order_statistic_gauss(\n",
    "        i, N) for i in range(1, N+1)]).T\n",
    "    a = (order_stats.dot(mat)).T\n",
    "    return a / sqrt(a.T.dot(a))\n",
    "\n",
    "def W_statistic(x: List[float], cached_coefficients: Optional[np.array] = None) -> float:\n",
    "    \"\"\"Calculate the W statistic for a sample.\n",
    "    \n",
    "    Args:\n",
    "        x: the sample\n",
    "        cached_coefficients: optionally cached Shapiro-Wilk coefficients\n",
    "    \n",
    "    Returns:\n",
    "        The W statistic\n",
    "    \"\"\"\n",
    "    N = len(x)\n",
    "    y = sorted(x)\n",
    "    a = cached_coefficients if cached_coefficients is not None else shapiro_wilk_coefficients(N)\n",
    "    x_bar = sum(x) / float(len(x))\n",
    "    return a.T.dot(np.array(y))**2 / sum([(xx - x_bar)**2 for xx in x]) \n",
    "             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run some Monte-Carlo simulations to figure out the significant values for $W$. The lower $W$ is, the more likely the sample is not from a normal distribution. The following will replicate Table 6 from the Shapiro-Wilk paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t0.01\t0.02\t0.05\t0.1\t0.5\t0.9\t0.95\t0.98\t0.99\n",
      "3\t0.507\t0.517\t0.550\t0.602\t0.934\t0.996\t1.001\t1.008\t1.015\n",
      "4\t0.501\t0.529\t0.579\t0.614\t0.840\t0.976\t0.989\t0.996\t0.999\n",
      "5\t0.518\t0.541\t0.579\t0.611\t0.776\t0.946\t0.968\t0.981\t0.988\n"
     ]
    }
   ],
   "source": [
    "def print_table_6(low: int = 3, hi: int = 50, nsamples: int=1000) -> None:\n",
    "    \"\"\"Print the data in Table 6 from the Sapiro Wilk 1964 paper.\"\"\"\n",
    "    levels = [0.01, 0.02, 0.05, 0.10, 0.50, 0.90, 0.95, 0.98, 0.99]\n",
    "    print(\"N\\t\" + \"\\t\".join([str(level) for level in levels]))\n",
    "    for N in range(low, hi+1):\n",
    "        a = shapiro_wilk_coefficients(N)\n",
    "        ws = [0.0 for _k in range(nsamples)]\n",
    "        for idx in range(nsamples):\n",
    "            sample = [gauss(0, 1) for _kk in range(N)]\n",
    "            ws[idx] = W_statistic(sample, cached_coefficients=a)\n",
    "        ws = sorted(ws)\n",
    "        cutoffs = [0 for _k in range(len(levels))]\n",
    "        for idx in range(len(levels)):\n",
    "            cutoffs[idx] = ws[int(nsamples*levels[idx])]\n",
    "        print(str(N)+\"\\t\"+\"\\t\".join([str(cutoff)[:5] for cutoff in cutoffs]))\n",
    "    return\n",
    "    \n",
    "print_table_6(hi=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing against the original paper, we can already see these numbers are pretty far off from those in the table from the paper. I suspect this is because of the error in the covariance matrix which is amplified by inversion. That said, with larger samples, this is exactly how Shapiro, Wilk, and H.J. Chen used in their 1968 paper examining the power of the test.\n",
    "\n",
    "TODO(gs): It looks like the cutoffs are always lower than they should be. Investigate why the error is asymmetric. \n",
    "\n",
    "Anyways, I might come back to this and go over some other tests of normality. Candidates include\n",
    "\n",
    "- Anderson-Darling\n",
    "- Kolmogorov-Smirnov\n",
    "- Shapiro-Francia"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
